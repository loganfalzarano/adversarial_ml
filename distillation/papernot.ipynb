{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.3025683538237613\n",
      "Epoch: 2\n",
      "Loss: 2.3025634603968053\n",
      "Epoch: 3\n",
      "Loss: 2.3025583361765976\n",
      "Epoch: 4\n",
      "Loss: 2.302555399917082\n",
      "Epoch: 5\n",
      "Loss: 2.302555424826486\n",
      "Epoch: 6\n",
      "Loss: 2.302549146640021\n",
      "Epoch: 7\n",
      "Loss: 2.302543472887865\n",
      "Epoch: 8\n",
      "Loss: 2.302541107511215\n",
      "Epoch: 9\n",
      "Loss: 2.302539542285618\n",
      "Epoch: 10\n",
      "Loss: 2.302536488596056\n",
      "Loss: 2.3044238332659004e-06\n",
      "Loss: 2.3048609727993607e-06\n",
      "Loss: 2.3041828535497187e-06\n",
      "Loss: 2.303927903994918e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, params[0], 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(params[0], params[1], 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(params[1], params[2], 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(params[2], params[3], 3, padding=1)\n",
    "        self.fc1 = nn.Linear(3136, params[4])\n",
    "        self.fc2 = nn.Linear(params[4], params[5])\n",
    "        self.fc3 = nn.Linear(params[5], 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        #flatten the input\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Train the model\n",
    "def train(data_loader, model, optimizer, num_epochs=50, train_temp=1, device='mps'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch:', epoch+1)\n",
    "        epoch_loss = 0\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = nn.CrossEntropyLoss()(outputs/10, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print('Loss:', epoch_loss/len(data_loader))\n",
    "            \n",
    "\n",
    "# Train the teacher model\n",
    "def train_teacher(data_loader, params, num_epochs=50, batch_size=128, train_temp=30, device='mps'):\n",
    "    model = Net(params).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
    "    train(data_loader, model, optimizer, num_epochs, train_temp, device)\n",
    "    return model\n",
    "\n",
    "# Train the student model using defensive distillation\n",
    "def train_distillation(data_loader, params, num_epochs=50, batch_size=128, train_temp=1, device='mps'):\n",
    "    teacher = train_teacher(data_loader, params, num_epochs, batch_size, train_temp, device)\n",
    "    teacher.eval()\n",
    "    student = Net(params).to(device)\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
    "\n",
    "    # Evaluate the teacher's predictions on the training data\n",
    "    student_data = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist_data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    teacher_outputs = []\n",
    "    for images, _ in student_data:\n",
    "        images = images.to(device)\n",
    "        outputs = teacher(images)\n",
    "        teacher_outputs.append(outputs.detach())\n",
    "    teacher_outputs = torch.cat(teacher_outputs, dim=0)\n",
    "\n",
    "    # Train the student using the teacher's outputs\n",
    "    student_data = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist_data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    counter = 0\n",
    "    loss_accum = 0\n",
    "    for images, _ in student_data:\n",
    "        images, teacher_outputs_batch = images.to(device), teacher_outputs[:images.size(0)].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = student(images)\n",
    "        loss = nn.KLDivLoss(reduction='batchmean')(nn.LogSoftmax(dim=1)(outputs/30), nn.Softmax(dim=1)(teacher_outputs_batch/30))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        teacher_outputs = teacher_outputs[images.size(0):]\n",
    "        loss_accum += loss.item()\n",
    "        counter += 1\n",
    "        if counter %100 == 0:\n",
    "            print('Loss:', loss_accum/counter)\n",
    "        \n",
    "    #write parameters to file\n",
    "    torch.save(student.state_dict(), 'student.pt')\n",
    "    torch.save(teacher.state_dict(), 'teacher.pt')\n",
    "        \n",
    "    return teacher, student\n",
    "\n",
    "# Example usage\n",
    "t, s = train_distillation(torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist_data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=True), [32, 32, 64, 64, 200, 200], num_epochs=10, train_temp=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist dataset\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "mnist_train.data = mnist_train.data[:5000]\n",
    "mnist_train.targets = mnist_train.targets[:5000]\n",
    "\n",
    "mnist_test.data = mnist_test.data[:500]\n",
    "mnist_test.targets = mnist_test.targets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = nn.functional.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect ``datagrad``\n",
    "        data_grad = data.grad.data\n",
    "        \n",
    "        def fgsm_attack(image, epsilon, data_grad):\n",
    "            # Collect the element-wise sign of the data gradient\n",
    "            sign_data_grad = data_grad.sign()\n",
    "            # Create the perturbed image by adjusting each pixel of the input image\n",
    "            perturbed_image = image + epsilon*sign_data_grad\n",
    "            # Adding clipping to maintain [0,1] range\n",
    "            perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "            # Return the perturbed image\n",
    "            return perturbed_image\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0 and len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model prediction: tensor(6, device='mps:0') Actual label: 6\n",
      "Student model prediction: tensor(7, device='mps:0') Actual label: 7\n",
      "Student model prediction: tensor(5, device='mps:0') Actual label: 5\n",
      "Student model prediction: tensor(0, device='mps:0') Actual label: 0\n",
      "Student model prediction: tensor(2, device='mps:0') Actual label: 2\n",
      "Student model prediction: tensor(4, device='mps:0') Actual label: 4\n",
      "Student model prediction: tensor(1, device='mps:0') Actual label: 1\n",
      "Student model prediction: tensor(3, device='mps:0') Actual label: 3\n",
      "Student model prediction: tensor(3, device='mps:0') Actual label: 3\n",
      "Student model prediction: tensor(8, device='mps:0') Actual label: 8\n"
     ]
    }
   ],
   "source": [
    "#get the student model predictions for first 10 images\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('Student model prediction:', torch.argmax(t(images.to('mps'))), 'Actual label:', labels.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
